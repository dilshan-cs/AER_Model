{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd538b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: evaluate in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: filelock in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in f:\\sem eval 12\\t5_aer\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8beb0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e82e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\":\"train.jsonl\",\n",
    "        \"validation\" : \"validation.jsonl\"\n",
    "\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225ca0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "\n",
    "def convert_multi_label(example):\n",
    "    labels = [0, 0, 0, 0]  # A,B,C,D\n",
    "\n",
    "    answers = example[\"golden_answer\"].split(\",\")\n",
    "\n",
    "    for ans in answers:\n",
    "        ans = ans.strip()\n",
    "        labels[label_map[ans]] = float(1)\n",
    "\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a721b470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a61243c2c44a2bb20f392d968dd28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e403e27c301e42079746d8964170ebe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(convert_multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cae2e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd22781e11e746c8bf054a282b41ccaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f4778354ea4875becd405e1f655b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_input(example):\n",
    "    text = (\n",
    "        f\"Target event: {example['target_event']}\\n\"\n",
    "        f\"A: {example['option_A']}\\n\"\n",
    "        f\"B: {example['option_B']}\\n\"\n",
    "        f\"C: {example['option_C']}\\n\"\n",
    "        f\"D: {example['option_D']}\\n\"\n",
    "        \"Which options are the plausible causes?\"\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "data = data.map(build_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "270e5469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103b80e771a545a0a0cc691466f030df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd366c3fc194fdd9d9931d7d76b6d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized = data.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "025533e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenized.remove_columns([\n",
    "    \"text\", \"topic_id\", \"uuid\",\n",
    "    \"target_event\", \"option_A\", \"option_B\",\n",
    "    \"option_C\", \"option_D\", \"golden_answer\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0804f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab9cdee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels = 4,\n",
    "    problem_type = \"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d2fed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./aer_multilabled_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dc0bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdils\\AppData\\Local\\Temp\\ipykernel_19068\\485827495.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take much time, 250 min atleast\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79730b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"aer_multilabel_roberta\")\n",
    "tokenizer.save_pretrained(\"aer_multilabel_roberta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_multilabel(target_event, A, B, C, D, threshold=0.5):\n",
    "\n",
    "    text = (\n",
    "        f\"Target event: {target_event}\\n\"\n",
    "        f\"A: {A}\\n\"\n",
    "        f\"B: {B}\\n\"\n",
    "        f\"C: {C}\\n\"\n",
    "        f\"D: {D}\\n\"\n",
    "        \"Which options are the plausible causes?\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits)[0]\n",
    "\n",
    "    labels = []\n",
    "    options = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    for i, p in enumerate(probs):\n",
    "        if p >= threshold:\n",
    "            labels.append(options[i])\n",
    "\n",
    "    return labels, probs.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers, probs = predict_multilabel(\n",
    "    \"South Korea’s parliament voted to impeach President Yoon Suk Yeol.\",\n",
    "    \"Yoon's senior aides and defense minister offered to resign\",\n",
    "    \"Hundreds of soldiers stormed the National Assembly\",\n",
    "    \"Tens of thousands of protesters gathered outside the National Assembly in Seoul\",\n",
    "    \"President Yoon Suk Yeol declared martial law on December 3 and sent soldiers to parliament\"\n",
    ")\n",
    "\n",
    "print(\"Predicted:\", answers)\n",
    "print(\"Probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13410a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef78b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model and make input \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_path = \"aer_multilabel_roberta\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  \n",
    "\n",
    "\n",
    "def predict_multilabel1(target_event, A, B, C, D, threshold=0.5):\n",
    "\n",
    "    text = (\n",
    "        f\"Target event: {target_event}\\n\"\n",
    "        f\"A: {A}\\n\"\n",
    "        f\"B: {B}\\n\"\n",
    "        f\"C: {C}\\n\"\n",
    "        f\"D: {D}\\n\"\n",
    "        \"Which options are the plausible causes?\"\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Run model prediction\n",
    "    with torch.no_grad():  # prevent gradients\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Convert logits → probabilities\n",
    "    probs = torch.sigmoid(logits)[0]\n",
    "\n",
    "    # Map probabilities to labels\n",
    "    labels = []\n",
    "    labels_binary = []\n",
    "    options = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    \n",
    "    for i, p in enumerate(probs):\n",
    "        if p >= threshold:\n",
    "            labels.append(options[i])\n",
    "            labels_binary.append(1)\n",
    "        else:\n",
    "            labels_binary.append(0)\n",
    "\n",
    "    return labels,labels_binary, probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ce47e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4165e5d0136b400caec597385f3eb5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = load_dataset(\"json\", data_files = {\"train\":\"validation_for_accuracy.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3655f9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1224676e55804cf98400db565a1f0c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = data2.map(convert_multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbca2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data from data2\n",
    "\n",
    "ansPredicted = []\n",
    "for i in range (0,len(data2[\"train\"])):\n",
    "    ans,ans_label,prob = predict_multilabel1(\n",
    "        data2[\"train\"][i][\"target_event\"],\n",
    "        data2[\"train\"][i][\"option_A\"],\n",
    "        data2[\"train\"][i][\"option_B\"],\n",
    "        data2[\"train\"][i][\"option_C\"],\n",
    "        data2[\"train\"][i][\"option_D\"] \n",
    "    )\n",
    "    ansPredicted.append(ans_label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9f01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted answers with actuals\n",
    "count = 0\n",
    "for i in range (0,len(ansPredicted)):\n",
    "    if   data2[\"train\"][\"labels\"][i] == ansPredicted[i]:\n",
    "        count+=1\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76e714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.75 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = count / len(ansPredicted) * 100\n",
    "print(accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b704b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
